
@misc{wang_review_2022,
	title = {A {Review} on {Graph} {Neural} {Network} {Methods} in {Financial} {Applications}},
	url = {http://arxiv.org/abs/2111.15367},
	doi = {10.48550/arXiv.2111.15367},
	abstract = {With multiple components and relations, financial data are often presented as graph data, since it could represent both the individual features and the complicated relations. Due to the complexity and volatility of the financial market, the graph constructed on the financial data is often heterogeneous or time-varying, which imposes challenges on modeling technology. Among the graph modeling technologies, graph neural network (GNN) models are able to handle the complex graph structure and achieve great performance and thus could be used to solve financial tasks. In this work, we provide a comprehensive review of GNN models in recent financial context. We first categorize the commonly-used financial graphs and summarize the feature processing step for each node. Then we summarize the GNN methodology for each graph type, application in each area, and propose some potential research areas.},
	urldate = {2025-09-03},
	publisher = {arXiv},
	author = {Wang, Jianian and Zhang, Sheng and Xiao, Yanghua and Song, Rui},
	month = apr,
	year = {2022},
	note = {arXiv:2111.15367 [q-fin]},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Statistical Finance, Statistics - Applications},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/GMD2E7A7/Wang et al. - 2022 - A Review on Graph Neural Network Methods in Financial Applications.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/4WX6JB24/2111.html:text/html},
}

@misc{kong_generative_2023,
	title = {Generative {Models} for {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/2302.13408},
	doi = {10.48550/arXiv.2302.13408},
	abstract = {Point clouds are rich geometric data structures, where their three dimensional structure offers an excellent domain for understanding the representation learning and generative modeling in 3D space. In this work, we aim to improve the performance of point cloud latent-space generative models by experimenting with transformer encoders, latent-space flow models, and autoregressive decoders. We analyze and compare both generation and reconstruction performance of these models on various object types.},
	urldate = {2025-10-11},
	publisher = {arXiv},
	author = {Kong, Lingjie and Rajak, Pankaj and Shakeri, Siamak},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13408 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/U3QI4PCX/Kong et al. - 2023 - Generative Models for 3D Point Clouds.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/DLJKIELP/2302.html:text/html},
}

@misc{kong_generative_2023-1,
	title = {Generative {Models} for {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/2302.13408},
	doi = {10.48550/arXiv.2302.13408},
	abstract = {Point clouds are rich geometric data structures, where their three dimensional structure offers an excellent domain for understanding the representation learning and generative modeling in 3D space. In this work, we aim to improve the performance of point cloud latent-space generative models by experimenting with transformer encoders, latent-space flow models, and autoregressive decoders. We analyze and compare both generation and reconstruction performance of these models on various object types.},
	urldate = {2025-10-11},
	publisher = {arXiv},
	author = {Kong, Lingjie and Rajak, Pankaj and Shakeri, Siamak},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13408 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/sspiegel/Zotero/storage/8Q3XRWAK/Kong et al. - 2023 - Generative Models for 3D Point Clouds.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/CLESAH66/2302.html:text/html},
}

@article{thomas_learning_nodate,
	title = {Learning new representations for {3D} point cloud semantic segmentation},
	language = {en},
	author = {Thomas, Hugues},
	file = {PDF:/home/sspiegel/Zotero/storage/JATS2XGC/Thomas - Learning new representations for 3D point cloud semantic segmentation.pdf:application/pdf},
}

@article{han_whu-urban3d_2024,
	title = {{WHU}-{Urban3D}: {An} urban scene {LiDAR} point cloud dataset for semantic instance segmentation},
	volume = {209},
	issn = {0924-2716},
	shorttitle = {{WHU}-{Urban3D}},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271624000522},
	doi = {10.1016/j.isprsjprs.2024.02.007},
	abstract = {With the rapid advancement of 3D sensors, there is an increasing demand for 3D scene understanding and an increasing number of 3D deep learning algorithms have been proposed. However, a large-scale and richly annotated 3D point cloud dataset is critical to understanding complicated road and urban scenes. Motivated by the need to bridge the gap between the rising demand for 3D urban scene understanding and limited LiDAR point cloud datasets, this paper proposes a richly annotated WHU-Urban3D dataset and an effective method for semantic instance segmentation. WHU-Urban3D stands out from existing datasets due to its distinctive features: (1) extensive coverage of both Airborne Laser Scanning and Mobile Laser Scanning point clouds, along with panoramic images; (2) containing large-scale road and urban scenes in different cities (over 3.2×106m2 area), with richly point-wise semantic instance labels (over 200 million points); (3) inclusion of particular attributes (e.g., reflected intensity, number of returns) in addition to 3D coordinates. This paper also provides the performance of several representative baseline methods and outlines potential future works and challenges for fully exploiting this dataset. The WHU-Urban3D dataset is publicly accessible at https://whu3d.com/.},
	urldate = {2025-10-13},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Han, Xu and Liu, Chong and Zhou, Yuzhou and Tan, Kai and Dong, Zhen and Yang, Bisheng},
	month = mar,
	year = {2024},
	keywords = {Machine learning, Point cloud, Semantic instance segmentation, Urban-scale dataset},
	pages = {500--513},
	file = {ScienceDirect Snapshot:/home/sspiegel/Zotero/storage/5WHCA6QZ/S0924271624000522.html:text/html},
}

@article{chakraborty_segmentation_2024,
	title = {Segmentation of {LiDAR} point cloud data in urban areas using adaptive neighborhood selection technique},
	volume = {19},
	issn = {1932-6203},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11257351/},
	doi = {10.1371/journal.pone.0307138},
	abstract = {Semantic segmentation of urban areas using Light Detection and Ranging (LiDAR) point cloud data is challenging due to the complexity, outliers, and heterogeneous nature of the input point cloud data. The machine learning-based methods for segmenting point clouds suffer from the imprecise computation of the training feature values. The most important factor that influences how precisely the feature values are computed is the neighborhood chosen by each point. This research addresses this issue and proposes a suitable adaptive neighborhood selection approach for individual points by completely considering the complex and heterogeneous nature of the input LiDAR point cloud data. The proposed approach is evaluated on high-density mobile and low-density aerial LiDAR point cloud datasets using the Random Forest machine learning classifier. In the context of performance evaluation, the proposed approach confirms the competitive performance over the state-of-the-art approaches. The computed accuracy and F1-score for the high-density Toronto and low-density Vaihingen datasets are greater than 91\% and 82\%, respectively.},
	number = {7},
	urldate = {2025-10-13},
	journal = {PLOS ONE},
	author = {Chakraborty, Debobrata and Dey, Emon Kumar},
	month = jul,
	year = {2024},
	pmid = {39024214},
	pmcid = {PMC11257351},
	pages = {e0307138},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/EYATSKS9/Chakraborty and Dey - 2024 - Segmentation of LiDAR point cloud data in urban areas using adaptive neighborhood selection techniqu.pdf:application/pdf},
}

@article{kuprowski_feature_2023,
	title = {Feature {Selection} for {Airbone} {LiDAR} {Point} {Cloud} {Classification}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/15/3/561},
	doi = {10.3390/rs15030561},
	abstract = {The classification of airborne LiDAR data is a prerequisite for many spatial data elaborations and analysis. In the domain of power supply networks, it is of utmost importance to be able to discern at least five classes for further processing—ground, buildings, vegetation, poles, and catenaries. This process is mainly performed manually by domain experts with the use of advanced point cloud manipulation software. The goal of this paper is to find a set of features which would divide space well enough to achieve accurate automatic classification on all relevant classes within the domain, thus reducing manual labor. To tackle this problem, we propose a single multi-class approach to classify all four basic classes (excluding ground) in a power supply domain with single pass-through, using one network. The proposed solution implements random forests and gradient boosting to create a feature-based per-point classifier which achieved an accuracy and F1 score of over 99\% on all tested cases, with the maximum of 99.7\% for accuracy and 99.5\% for F1 score. Moreover, we achieved a maximum of 81.7\% F1 score for the most sparse class. The results show that the proposed set of features for the LiDAR data cloud is effective in power supply line classification.},
	language = {en},
	number = {3},
	urldate = {2025-10-13},
	journal = {Remote Sensing},
	author = {Kuprowski, Mateusz and Drozda, Pawel},
	month = jan,
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {feature selection, LiDAR, multi-scale neighborhood features, power supply network classification, random forests, XGBoost},
	pages = {561},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/G8BN6WAV/Kuprowski and Drozda - 2023 - Feature Selection for Airbone LiDAR Point Cloud Classification.pdf:application/pdf},
}

@inproceedings{thomas_semantic_2018,
	address = {Verona},
	title = {Semantic {Classification} of {3D} {Point} {Clouds} with {Multiscale} {Spherical} {Neighborhoods}},
	isbn = {978-1-5386-8425-2},
	url = {https://ieeexplore.ieee.org/document/8490990/},
	doi = {10.1109/3DV.2018.00052},
	abstract = {This paper introduces a new deﬁnition of multiscale neighborhoods in 3D point clouds. This deﬁnition, based on spherical neighborhoods and proportional subsampling, allows the computation of features with a consistent geometrical meaning, which is not the case when using k-nearest neighbors. With an appropriate learning strategy, the proposed features can be used in a random forest to classify 3D points. In this semantic classiﬁcation task, we show that our multiscale features outperform state-of-the-art features using the same experimental conditions. Furthermore, their classiﬁcation power competes with more elaborate classiﬁcation approaches including Deep Learning methods.},
	language = {en},
	urldate = {2025-10-14},
	booktitle = {2018 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Thomas, Hugues and Goulette, Francois and Deschaud, Jean-Emmanuel and Marcotegui, Beatriz and LeGall, Yann},
	month = sep,
	year = {2018},
	pages = {390--398},
	file = {PDF:/home/sspiegel/Zotero/storage/2WKC3BHE/Thomas et al. - 2018 - Semantic Classification of 3D Point Clouds with Multiscale Spherical Neighborhoods.pdf:application/pdf},
}

@misc{noauthor_2020_nodate,
	title = {2020 {LiDAR} - {Classified} {LAS}},
	url = {https://opendata.dc.gov/datasets/DCGIS::2020-lidar-classified-las/about},
	abstract = {This data is used for the planning and management of Washington, D.C. by local government agencies.},
	language = {en-us},
	urldate = {2025-10-17},
	file = {Snapshot:/home/sspiegel/Zotero/storage/ZS67RP74/about.html:text/html},
}

@misc{noauthor_kpconv_nodate,
	title = {{KPConv}: {Flexible} and {Deformable} {Convolution} for {Point} {Clouds}},
	shorttitle = {{KPConv}},
	url = {https://ar5iv.labs.arxiv.org/html/1904.08889},
	abstract = {We present Kernel Point Convolution111Project page: https://github.com/HuguesTHOMAS/KPConv (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The co…},
	language = {en},
	urldate = {2025-10-22},
	journal = {ar5iv},
	file = {Snapshot:/home/sspiegel/Zotero/storage/DIIHWCYN/1904.html:text/html},
}

@inproceedings{tan_toronto-3d_2020,
	address = {Seattle, WA, USA},
	title = {Toronto-{3D}: {A} {Large}-scale {Mobile} {LiDAR} {Dataset} for {Semantic} {Segmentation} of {Urban} {Roadways}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-9360-1},
	shorttitle = {Toronto-{3D}},
	url = {https://ieeexplore.ieee.org/document/9150609/},
	doi = {10.1109/CVPRW50498.2020.00109},
	abstract = {Semantic segmentation of large-scale outdoor point clouds is essential for urban scene understanding in various applications, especially autonomous driving and urban high-deﬁnition (HD) mapping. With rapid developments of mobile laser scanning (MLS) systems, massive point clouds are available for scene understanding, but publicly accessible large-scale labeled datasets, which are essential for developing learning-based methods, are still limited. This paper introduces Toronto-3D, a large-scale urban outdoor point cloud dataset acquired by a MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of point clouds and consists of about 78.3 million points with 8 labeled object classes. Baseline experiments for semantic segmentation were conducted and the results conﬁrmed the capability of this dataset to train deep learning models effectively. Toronto-3D is released 1 to encourage new research, and the labels will be improved and updated with feedback from the research community.},
	language = {en},
	urldate = {2025-10-24},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Tan, Weikai and Qin, Nannan and Ma, Lingfei and Li, Ying and Du, Jing and Cai, Guorong and Yang, Ke and Li, Jonathan},
	month = jun,
	year = {2020},
	pages = {797--806},
	file = {PDF:/home/sspiegel/Zotero/storage/DEYM2XCJ/Tan et al. - 2020 - Toronto-3D A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways.pdf:application/pdf},
}

@misc{noauthor_illinois_nodate,
	title = {Illinois {Height} {Modernization} ({ILHMP}): {LiDAR} {Data} {\textbar} clearinghouse.isgs.illinois.edu},
	url = {https://clearinghouse.isgs.illinois.edu/data/elevation/illinois-height-modernization-ilhmp},
	urldate = {2025-10-24},
	file = {Illinois Height Modernization (ILHMP)\: LiDAR Data | clearinghouse.isgs.illinois.edu:/home/sspiegel/Zotero/storage/J95SIXG3/illinois-height-modernization-ilhmp.html:text/html},
}

@article{rusu_semantic_2010,
	title = {Semantic {3D} {Object} {Maps} for {Everyday} {Manipulation} in {Human} {Living} {Environments}},
	volume = {24},
	copyright = {http://www.springer.com/tdm},
	issn = {0933-1875, 1610-1987},
	url = {http://link.springer.com/10.1007/s13218-010-0059-6},
	doi = {10.1007/s13218-010-0059-6},
	language = {en},
	number = {4},
	urldate = {2025-10-24},
	journal = {KI - Künstliche Intelligenz},
	author = {Rusu, Radu Bogdan},
	month = nov,
	year = {2010},
	pages = {345--348},
	file = {PDF:/home/sspiegel/Zotero/storage/27TKAHAE/Rusu - 2010 - Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments.pdf:application/pdf},
}

@article{rusu_semantic_2010-1,
	title = {Semantic {3D} {Object} {Maps} for {Everyday} {Manipulation} in {Human} {Living} {Environments}},
	volume = {24},
	issn = {1610-1987},
	url = {https://doi.org/10.1007/s13218-010-0059-6},
	doi = {10.1007/s13218-010-0059-6},
	abstract = {Environment models serve as important resources for an autonomous robot by providing it with the necessary task-relevant information about its habitat. Their use enables robots to perform their tasks more reliably, flexibly, and efficiently. As autonomous robotic platforms get more sophisticated manipulation capabilities, they also need more expressive and comprehensive environment models: for manipulation purposes their models have to include the objects present in the world, together with their position, form, and other aspects, as well as an interpretation of these objects with respect to the robot tasks.},
	language = {en},
	number = {4},
	urldate = {2025-10-24},
	journal = {KI - Künstliche Intelligenz},
	author = {Rusu, Radu Bogdan},
	month = nov,
	year = {2010},
	keywords = {Machine Learning Classifier, Perception Problem, Point Cloud, Robot Hexapod, Robot Operating System},
	pages = {345--348},
}

@article{filin_neighborhood_2005,
	title = {Neighborhood {Systems} for {Airborne} {Laser} {Data}},
	volume = {71},
	doi = {10.14358/PERS.71.6.743},
	abstract = {Analysis of common neighborhood definitions for airborne laser data, triangulation or raster-based, reveals deficiencies in modeling the measured objects. Concepts that originate from 2D data structures are used for modeling complex 3D objects and for handling datasets with different
point densities. Realizing these shortcomings, this paper proposes a new neighborhood system for airborne laser data. Based on laser data characteristics the proposed systems consider, among other features, point density, layered and overhanging structures, and local surface trends. Parameters
for the proposed systems are derived from theoretical and practical observations. The paper demonstrates the type of neighborhood that is established by the proposed systems, and shows that artifacts that are usually created by the common neighborhoods are avoided here, and that structures
within the data that are usually masked are revealed. The paper demonstrates how subsequent applications benefit from the new system. Finally, the estimation of surface normals by the proposed systems is compared to the triangulation; results show a significant improvement in the reliability
and quality of the estimation.},
	number = {6},
	journal = {Photogrammetric Engineering \& Remote Sensing},
	author = {Filin, Sagi and Pfeifer, Norbert},
	month = jun,
	year = {2005},
	pages = {743--755},
	file = {IngentaConnect Full Text PDF:/home/sspiegel/Zotero/storage/KIUU5SAQ/Filin and Pfeifer - 2005 - Neighborhood Systems for Airborne Laser Data.pdf:application/pdf},
}

@article{weinmann_semantic_2015,
	title = {Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers},
	volume = {105},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271615000349},
	doi = {10.1016/j.isprsjprs.2015.01.016},
	abstract = {3D scene analysis in terms of automatically assigning 3D points a respective semantic label has become a topic of great importance in photogrammetry, remote sensing, computer vision and robotics. In this paper, we address the issue of how to increase the distinctiveness of geometric features and select the most relevant ones among these for 3D scene analysis. We present a new, fully automated and versatile framework composed of four components: (i) neighborhood selection, (ii) feature extraction, (iii) feature selection and (iv) classification. For each component, we consider a variety of approaches which allow applicability in terms of simplicity, efficiency and reproducibility, so that end-users can easily apply the different components and do not require expert knowledge in the respective domains. In a detailed evaluation involving 7 neighborhood definitions, 21 geometric features, 7 approaches for feature selection, 10 classifiers and 2 benchmark datasets, we demonstrate that the selection of optimal neighborhoods for individual 3D points significantly improves the results of 3D scene analysis. Additionally, we show that the selection of adequate feature subsets may even further increase the quality of the derived results while significantly reducing both processing time and memory consumption.},
	urldate = {2025-11-05},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Weinmann, Martin and Jutzi, Boris and Hinz, Stefan and Mallet, Clément},
	month = jul,
	year = {2015},
	keywords = {Point cloud, 3D scene analysis, Classification, Feature extraction, Feature selection, Neighborhood selection},
	pages = {286--304},
	file = {ScienceDirect Snapshot:/home/sspiegel/Zotero/storage/FZAPWJEB/S0924271615000349.html:text/html},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Segmentation} of {Planar} {Surfaces} from {Laser} {Scanning} {Data} {Using} the {Magnitude} of {Normal} {Position} {Vector} for {Adaptive} {Neighborhoods}},
	url = {https://www.researchgate.net/publication/291530029_Segmentation_of_Planar_Surfaces_from_Laser_Scanning_Data_Using_the_Magnitude_of_Normal_Position_Vector_for_Adaptive_Neighborhoods?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ},
	urldate = {2025-10-30},
}

@article{chakraborty_segmentation_2024-1,
	title = {Segmentation of {LiDAR} point cloud data in urban areas using adaptive neighborhood selection technique},
	volume = {19},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0307138},
	doi = {10.1371/journal.pone.0307138},
	abstract = {Semantic segmentation of urban areas using Light Detection and Ranging (LiDAR) point cloud data is challenging due to the complexity, outliers, and heterogeneous nature of the input point cloud data. The machine learning-based methods for segmenting point clouds suffer from the imprecise computation of the training feature values. The most important factor that influences how precisely the feature values are computed is the neighborhood chosen by each point. This research addresses this issue and proposes a suitable adaptive neighborhood selection approach for individual points by completely considering the complex and heterogeneous nature of the input LiDAR point cloud data. The proposed approach is evaluated on high-density mobile and low-density aerial LiDAR point cloud datasets using the Random Forest machine learning classifier. In the context of performance evaluation, the proposed approach confirms the competitive performance over the state-of-the-art approaches. The computed accuracy and F1-score for the high-density Toronto and low-density Vaihingen datasets are greater than 91\% and 82\%, respectively.},
	language = {en},
	number = {7},
	urldate = {2025-10-30},
	journal = {PLOS ONE},
	author = {Chakraborty, Debobrata and Dey, Emon Kumar},
	month = jul,
	year = {2024},
	note = {Publisher: Public Library of Science},
	keywords = {Machine learning, Curvature, Deep learning, Eigenvalues, Lidar, Neighborhoods, Radii, Urban areas},
	pages = {e0307138},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/GWX4SRPR/Chakraborty and Dey - 2024 - Segmentation of LiDAR point cloud data in urban areas using adaptive neighborhood selection techniqu.pdf:application/pdf},
}

@article{singer_dales_2021,
	title = {{DALES} {Objects}: {A} {Large} {Scale} {Benchmark} {Dataset} for {Instance} {Segmentation} in {Aerial} {Lidar}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {{DALES} {Objects}},
	url = {https://ieeexplore.ieee.org/document/9469802},
	doi = {10.1109/ACCESS.2021.3094127},
	abstract = {We present DALES Objects, a large-scale instance segmentation benchmark dataset for aerial lidar. DALES Objects contains close to half a billion hand-labeled points, including semantic and instance segmentation labels. DALES Objects is an extension of the DALES (Varney et al., 2020) dataset, adding additional intensity and instance segmentation annotation. This paper provides an overview of the data collection, preprocessing, hand-labeling strategy, and final data format. We propose relevant evaluation metrics and provide insights into potential challenges when evaluating this benchmark dataset. Finally, we provide information about how researchers can access the dataset for their use at go.udayton.edu/dales3d.},
	urldate = {2025-10-30},
	journal = {IEEE Access},
	author = {Singer, Nina M. and Asari, Vijayan K.},
	year = {2021},
	keywords = {Deep learning, 3D data set, aerial vision, airborne system, ALS, benchmark data, Benchmark testing, data annotation, deep learning, earth scan, instance segmentation, Laser radar, laser scan, lidar, point cloud, semantic segmentation, Semantics, Task analysis, Three-dimensional displays, Vegetation mapping},
	pages = {97495--97504},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/3FQEJ2WN/Singer and Asari - 2021 - DALES Objects A Large Scale Benchmark Dataset for Instance Segmentation in Aerial Lidar.pdf:application/pdf},
}

@inproceedings{lin_pointacc_2021,
	title = {{PointAcc}: {Efficient} {Point} {Cloud} {Accelerator}},
	shorttitle = {{PointAcc}},
	url = {http://arxiv.org/abs/2110.07600},
	doi = {10.1145/3466752.3480084},
	abstract = {Deep learning on point clouds plays a vital role in a wide range of applications such as autonomous driving and AR/VR. These applications interact with people in real time on edge devices and thus require low latency and low energy. Compared to projecting the point cloud to 2D space, directly processing 3D point cloud yields higher accuracy and lower \#MACs. However, the extremely sparse nature of point cloud poses challenges to hardware acceleration. For example, we need to explicitly determine the nonzero outputs and search for the nonzero neighbors (mapping operation), which is unsupported in existing accelerators. Furthermore, explicit gather and scatter of sparse features are required, resulting in large data movement overhead.},
	language = {en},
	urldate = {2025-10-30},
	booktitle = {{MICRO}-54: 54th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	author = {Lin, Yujun and Zhang, Zhekai and Tang, Haotian and Wang, Hanrui and Han, Song},
	month = oct,
	year = {2021},
	note = {arXiv:2110.07600 [cs]},
	keywords = {Computer Science - Hardware Architecture},
	pages = {449--461},
	file = {PDF:/home/sspiegel/Zotero/storage/ALLGN7NI/Lin et al. - 2021 - PointAcc Efficient Point Cloud Accelerator.pdf:application/pdf},
}

@misc{roynard_paris-lille-3d_2017,
	title = {Paris-{Lille}-{3D}: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification},
	shorttitle = {Paris-{Lille}-{3D}},
	url = {https://arxiv.org/abs/1712.00032v2},
	abstract = {This paper introduces a new Urban Point Cloud Dataset for Automatic Segmentation and Classification acquired by Mobile Laser Scanning (MLS). We describe how the dataset is obtained from acquisition to post-processing and labeling. This dataset can be used to learn classification algorithm, however, given that a great attention has been paid to the split between the different objects, this dataset can also be used to learn the segmentation. The dataset consists of around 2km of MLS point cloud acquired in two cities. The number of points and range of classes make us consider that it can be used to train Deep-Learning methods. Besides we show some results of automatic segmentation and classification. The dataset is available at: http://caor-mines-paristech.fr/fr/paris-lille-3d-dataset/},
	language = {en},
	urldate = {2025-11-11},
	journal = {arXiv.org},
	author = {Roynard, Xavier and Deschaud, Jean-Emmanuel and Goulette, François},
	month = nov,
	year = {2017},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/PSLETCHL/Roynard et al. - 2017 - Paris-Lille-3D a large and high-quality ground truth urban point cloud dataset for automatic segmen.pdf:application/pdf},
}

@inproceedings{chu_neighbor-vote_2021,
	address = {Virtual Event China},
	title = {Neighbor-{Vote}: {Improving} {Monocular} {3D} {Object} {Detection} through {Neighbor} {Distance} {Voting}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {Neighbor-{Vote}},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475641},
	doi = {10.1145/3474085.3475641},
	language = {en},
	urldate = {2025-11-11},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Chu, Xiaomeng and Deng, Jiajun and Li, Yao and Yuan, Zhenxun and Zhang, Yanyong and Ji, Jianmin and Zhang, Yu},
	month = oct,
	year = {2021},
	pages = {5239--5247},
}

@article{mohamed_evaluation_2021,
	title = {Evaluation of data subsampling and neighbourhood selection for mobile {LiDAR} data classification},
	volume = {24},
	issn = {1110-9823},
	url = {https://www.sciencedirect.com/science/article/pii/S1110982321000363},
	doi = {10.1016/j.ejrs.2021.04.003},
	abstract = {Road features extraction is essential for autonomous driving vehicles and road maintenance. Mobile Laser Scanning (MLS) systems have proven their capability for dense and accurate LiDAR point cloud data acquisition of road features. Usually, MLS data are received in the format of XYZ coordinates and sometimes with intensity values. Thus, the first step in MLS data processing is point classification, which mainly relays on the geometric distribution of surrounding points. However, processing such huge data is costly and time- consuming. Therefore, in this research, different neighborhood selection methods, including k nearest neighbors, spherical and cylindrical methods are evaluated to reveal the suitable method for MLS data classification. In addition, a data sub-sampling method based on minimum point spacing is applied in order to reduce the processing time. A set of point features, including covariance, moment and height was first extracted based on the three neighborhood selection methods. Random forest classifier was then used to classify a part of the benchmark dataset of Paris–Lille-3D, which belongs to NPM3D Benchmark suite research project. The dataset is divided into three main parts; Lille 1, Lille 2 and Paris. Lille 1 and Lille 2 were used in this research with about 1.5 km longitudinal road and about 98.1 million total number of points. Six scenarios were evaluated; three for the full dataset and three for the sub-sampled dataset using the aforementioned neighborhood selection methods. The results showed that the cylindrical neighborhood selection method achieved the highest classification accuracy of 92.39\% and 90.26\% for the full and sub-sampled datasets, respectively. The data sub-sampling has showed a good performance, whereas the dataset was reduced by about half and processing time was reduced by almost half with close classification accuracy using the cylindrical neighborhood selection method.},
	number = {3, Part 2},
	urldate = {2025-11-11},
	journal = {The Egyptian Journal of Remote Sensing and Space Science},
	author = {Mohamed, Mahmoud and Morsy, Salem and El-Shazly, Adel},
	month = dec,
	year = {2021},
	keywords = {Classification, Neighborhood selection, Mobile LiDAR scanning, Random forest, Sub-sampling},
	pages = {799--804},
	file = {ScienceDirect Full Text PDF:/home/sspiegel/Zotero/storage/BFRURUJW/Mohamed et al. - 2021 - Evaluation of data subsampling and neighbourhood selection for mobile LiDAR data classification.pdf:application/pdf;ScienceDirect Snapshot:/home/sspiegel/Zotero/storage/NG4YCYG8/S1110982321000363.html:text/html},
}

@inproceedings{chu_neighbor-vote_2021-1,
	address = {Virtual Event China},
	title = {Neighbor-{Vote}: {Improving} {Monocular} {3D} {Object} {Detection} through {Neighbor} {Distance} {Voting}},
	isbn = {978-1-4503-8651-7},
	shorttitle = {Neighbor-{Vote}},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475641},
	doi = {10.1145/3474085.3475641},
	language = {en},
	urldate = {2025-11-11},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Chu, Xiaomeng and Deng, Jiajun and Li, Yao and Yuan, Zhenxun and Zhang, Yanyong and Ji, Jianmin and Zhang, Yu},
	month = oct,
	year = {2021},
	pages = {5239--5247},
}

@misc{thomas_kpconv_2019,
	title = {{KPConv}: {Flexible} and {Deformable} {Convolution} for {Point} {Clouds}},
	shorttitle = {{KPConv}},
	url = {http://arxiv.org/abs/1904.08889},
	doi = {10.48550/arXiv.1904.08889},
	abstract = {We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Thomas, Hugues and Qi, Charles R. and Deschaud, Jean-Emmanuel and Marcotegui, Beatriz and Goulette, François and Guibas, Leonidas J.},
	month = aug,
	year = {2019},
	note = {arXiv:1904.08889 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/PHE8N7PS/Thomas et al. - 2019 - KPConv Flexible and Deformable Convolution for Point Clouds.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/ERXP2Y8C/1904.html:text/html},
}

@misc{hu_randla-net_2020,
	title = {{RandLA}-{Net}: {Efficient} {Semantic} {Segmentation} of {Large}-{Scale} {Point} {Clouds}},
	shorttitle = {{RandLA}-{Net}},
	url = {http://arxiv.org/abs/1911.11236},
	doi = {10.48550/arXiv.1911.11236},
	abstract = {We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.},
	urldate = {2025-11-12},
	publisher = {arXiv},
	author = {Hu, Qingyong and Yang, Bo and Xie, Linhai and Rosa, Stefano and Guo, Yulan and Wang, Zhihua and Trigoni, Niki and Markham, Andrew},
	month = may,
	year = {2020},
	note = {arXiv:1911.11236 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Snapshot:/home/sspiegel/Zotero/storage/BW6FQWIJ/1911.html:text/html},
}

@article{sarker_comprehensive_2024,
	title = {A comprehensive overview of deep learning techniques for {3D} point cloud classification and semantic segmentation},
	volume = {35},
	issn = {0932-8092, 1432-1769},
	url = {http://arxiv.org/abs/2405.11903},
	doi = {10.1007/s00138-024-01543-1},
	abstract = {Point cloud analysis has a wide range of applications in many areas such as computer vision, robotic manipulation, and autonomous driving. While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unordered, irregular and noisy 3D points. To stimulate future research, this paper analyzes recent progress in deep learning methods employed for point cloud processing and presents challenges and potential directions to advance this field. It serves as a comprehensive review on two major tasks in 3D point cloud processing-- namely, 3D shape classification and semantic segmentation.},
	number = {4},
	urldate = {2025-11-20},
	journal = {Machine Vision and Applications},
	author = {Sarker, Sushmita and Sarker, Prithul and Stone, Gunner and Gorman, Ryan and Tavakkoli, Alireza and Bebis, George and Sattarvand, Javad},
	month = jul,
	year = {2024},
	note = {arXiv:2405.11903 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {67},
	file = {Preprint PDF:/home/sspiegel/Zotero/storage/BKBND2E3/Sarker et al. - 2024 - A comprehensive overview of deep learning techniques for 3D point cloud classification and semantic.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/8K5JDJ7F/2405.html:text/html},
}

@inproceedings{wu_3d_2015,
	title = {{3D} {ShapeNets}: {A} deep representation for volumetric shapes},
	shorttitle = {{3D} {ShapeNets}},
	url = {https://ieeexplore.ieee.org/document/7298801},
	doi = {10.1109/CVPR.2015.7298801},
	abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
	urldate = {2025-11-20},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Three-dimensional displays, Computational modeling, Convolution, Object recognition, Planning, Shape, Solid modeling},
	pages = {1912--1920},
	file = {Snapshot:/home/sspiegel/Zotero/storage/UIPBC7FA/7298801.html:text/html;Submitted Version:/home/sspiegel/Zotero/storage/XKF6JDCW/Wu et al. - 2015 - 3D ShapeNets A deep representation for volumetric shapes.pdf:application/pdf},
}

@misc{cicek_3d_2016,
	title = {{3D} {U}-{Net}: {Learning} {Dense} {Volumetric} {Segmentation} from {Sparse} {Annotation}},
	shorttitle = {{3D} {U}-{Net}},
	url = {http://arxiv.org/abs/1606.06650},
	doi = {10.48550/arXiv.1606.06650},
	abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {Çiçek, Özgün and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
	month = jun,
	year = {2016},
	note = {arXiv:1606.06650 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/7UP6G73S/Çiçek et al. - 2016 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/DFHIB6JD/1606.html:text/html},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/sspiegel/Zotero/storage/WGY3CVLL/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/WT3S6B3Q/1505.html:text/html},
}

@misc{noauthor_150504597_nodate,
	title = {[1505.04597] {U}-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	url = {https://arxiv.org/abs/1505.04597},
	urldate = {2025-11-26},
	file = {[1505.04597] U-Net\: Convolutional Networks for Biomedical Image Segmentation:/home/sspiegel/Zotero/storage/RJH9C4C6/1505.html:text/html},
}

@misc{ronneberger_u-net_2015-1,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/59DDITRY/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/VMAD2QDW/1505.html:text/html},
}

@article{rusu_semantic_2010-2,
	title = {Semantic {3D} {Object} {Maps} for {Everyday} {Manipulation} in {Human} {Living} {Environments}},
	volume = {24},
	copyright = {http://www.springer.com/tdm},
	issn = {0933-1875, 1610-1987},
	url = {http://link.springer.com/10.1007/s13218-010-0059-6},
	doi = {10.1007/s13218-010-0059-6},
	language = {en},
	number = {4},
	urldate = {2025-11-26},
	journal = {KI - Künstliche Intelligenz},
	author = {Rusu, Radu Bogdan},
	month = nov,
	year = {2010},
	pages = {345--348},
	file = {PDF:/home/sspiegel/Zotero/storage/MY5LM7VG/Rusu - 2010 - Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments.pdf:application/pdf},
}

@article{hackel_fast_2016,
	title = {{FAST} {SEMANTIC} {SEGMENTATION} {OF} {3D} {POINT} {CLOUDS} {WITH} {STRONGLY} {VARYING} {DENSITY}},
	volume = {III-3},
	url = {https://isprs-annals.copernicus.org/articles/III-3/177/2016/},
	doi = {10.5194/isprs-annals-III-3-177-2016},
	journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Hackel, T. and Wegner, J. D. and Schindler, K.},
	year = {2016},
	pages = {177--184},
}

@article{demantke_dimensionality_2011,
	title = {{DIMENSIONALITY} {BASED} {SCALE} {SELECTION} {IN} {3D} {LIDAR} {POINT} {CLOUDS}},
	volume = {XXXVIII-5/W12},
	url = {https://isprs-archives.copernicus.org/articles/XXXVIII-5-W12/97/2011/},
	doi = {10.5194/isprsarchives-XXXVIII-5-W12-97-2011},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Demantké, J. and Mallet, C. and David, N. and Vallet, B.},
	year = {2011},
	pages = {97--102},
}

@misc{zhou_open3d_2018,
	title = {{Open3D}: {A} {Modern} {Library} for {3D} {Data} {Processing}},
	shorttitle = {{Open3D}},
	url = {http://arxiv.org/abs/1801.09847},
	doi = {10.48550/arXiv.1801.09847},
	abstract = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
	month = jan,
	year = {2018},
	note = {arXiv:1801.09847 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Robotics},
	file = {Preprint PDF:/home/sspiegel/Zotero/storage/VRM8V95Z/Zhou et al. - 2018 - Open3D A Modern Library for 3D Data Processing.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/SJA9AD9N/1801.html:text/html},
}

@misc{noauthor_welcome_2025,
	title = {Welcome to {Python}.org},
	url = {https://www.python.org/},
	abstract = {The official home of the Python Programming Language},
	language = {en},
	urldate = {2025-11-27},
	journal = {Python.org},
	month = nov,
	year = {2025},
	file = {Snapshot:/home/sspiegel/Zotero/storage/DQ4VI9SE/www.python.org.html:text/html},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	number = {85},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	year = {2011},
	pages = {2825--2830},
}

@inproceedings{mckinney_data_2010,
	address = {Austin, Texas},
	title = {Data {Structures} for {Statistical} {Computing} in {Python}},
	url = {https://doi.curvenote.com/10.25080/Majora-92bf1922-00a},
	doi = {10.25080/Majora-92bf1922-00a},
	abstract = {In this paper we are concerned with the practical issues of working with data sets common to ﬁnance, statistics, and other related ﬁelds. pandas is a new library which aims to facilitate working with these data sets and to provide a set of fundamental building blocks for implementing statistical models. We will discuss speciﬁc design issues encountered in the course of developing pandas with relevant examples and some comparisons with the R language. We conclude by discussing possible future directions for statistical computing and data analysis using Python.},
	language = {en},
	urldate = {2025-11-27},
	author = {McKinney, Wes},
	year = {2010},
	pages = {56--61},
	file = {PDF:/home/sspiegel/Zotero/storage/AEKL2692/McKinney - 2010 - Data Structures for Statistical Computing in Python.pdf:application/pdf},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	language = {en},
	number = {7825},
	urldate = {2025-11-27},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	month = sep,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Computational science, Computer science, Software, Solar physics},
	pages = {357--362},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/VCW2BJCE/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf},
}

@misc{butler_pdalpdal_2024,
	title = {{PDAL}/{PDAL}: 2.7.1},
	shorttitle = {{PDAL}/{PDAL}},
	url = {https://zenodo.org/records/10884408},
	abstract = {Bug Fixes



Add more logging to readers.stac \#4353

Don't break strict aliasing rules \#4361

Don't do readers.las ready() processing if there are no points \#4363

readers.arrow and writers.arrow do not require GDAL CMake dependency \#4365

Add writers.tiledb duplicate points config \#4370

use CONFIG CMake find\_package for filters.arrow and readers.draco/writers.draco \#4369

Don't attempt tree isolation for isolated point in filters.litree \#4373


Changelog since 2.7.0: https://github.com/PDAL/PDAL/compare/2.7.0...2.7.1

Changelog between 2.7.0 and 2.7.1: https://github.com/PDAL/PDAL/compare/2.7.1...2.7-maintenance},
	urldate = {2025-11-27},
	publisher = {Zenodo},
	author = {Butler, Howard and Bell, Andrew and Gerlek, Michael P. and chambbj and Gadomski, Pete and Manning, Connor and Łoskot, Mateusz and Couwenberg, Bas and Barker, Norman and Ramsey, Paul and Dark, Julia and Chaulet, Nicolas and Rouault, Even and Mann, Kyle and Foster, Claire and Villemin, Guilhem and Rosen, Michael and Grigory and Moore, Ogi and Lewis, Scott and McKelvey, Kirk and Brookes, Daniel and Evers, Kristian and Dobias, Martin and Coup, Robert and Vergara, Vicky and xantares and Bram and Yonas, Aaron},
	month = mar,
	year = {2024},
	doi = {10.5281/zenodo.10884408},
	file = {Snapshot:/home/sspiegel/Zotero/storage/LMAQLTCP/10884408.html:text/html},
}

@misc{girardeau-montaut_cloudcompare_nodate,
	title = {{CloudCompare}},
	author = {Girardeau-Montaut, Daniel},
}

@article{zhang_deep_2023,
	title = {Deep {Learning}-based {3D} {Point} {Cloud} {Classification}: {A} {Systematic} {Survey} and {Outlook}},
	volume = {79},
	issn = {01419382},
	shorttitle = {Deep {Learning}-based {3D} {Point} {Cloud} {Classification}},
	url = {http://arxiv.org/abs/2311.02608},
	doi = {10.1016/j.displa.2023.102456},
	abstract = {In recent years, point cloud representation has become one of the research hotspots in the field of computer vision, and has been widely used in many fields, such as autonomous driving, virtual reality, robotics, etc. Although deep learning techniques have achieved great success in processing regular structured 2D grid image data, there are still great challenges in processing irregular, unstructured point cloud data. Point cloud classification is the basis of point cloud analysis, and many deep learning-based methods have been widely used in this task. Therefore, the purpose of this paper is to provide researchers in this field with the latest research progress and future trends. First, we introduce point cloud acquisition, characteristics, and challenges. Second, we review 3D data representations, storage formats, and commonly used datasets for point cloud classification. We then summarize deep learning-based methods for point cloud classification and complement recent research work. Next, we compare and analyze the performance of the main methods. Finally, we discuss some challenges and future directions for point cloud classification.},
	urldate = {2025-12-03},
	journal = {Displays},
	author = {Zhang, Huang and Wang, Changshuo and Tian, Shengwei and Lu, Baoli and Zhang, Liping and Ning, Xin and Bai, Xiao},
	month = sep,
	year = {2023},
	note = {arXiv:2311.02608 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {102456},
	file = {Preprint PDF:/home/sspiegel/Zotero/storage/H5W6BJ3I/Zhang et al. - 2023 - Deep Learning-based 3D Point Cloud Classification A Systematic Survey and Outlook.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/RQ7T94JF/2311.html:text/html},
}

@misc{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	doi = {10.48550/arXiv.1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	urldate = {2025-12-05},
	publisher = {arXiv},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv:1612.00593 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/sspiegel/Zotero/storage/JINI4RVC/Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Classification and Segmentation.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/LXA5FYJR/1612.html:text/html},
}

@misc{yu_point-bert_2022,
	title = {Point-{BERT}: {Pre}-training {3D} {Point} {Cloud} {Transformers} with {Masked} {Point} {Modeling}},
	shorttitle = {Point-{BERT}},
	url = {http://arxiv.org/abs/2111.14819},
	doi = {10.48550/arXiv.2111.14819},
	abstract = {We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8\% accuracy on ModelNet40 and 83.1\% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT},
	urldate = {2025-12-05},
	publisher = {arXiv},
	author = {Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen},
	month = jun,
	year = {2022},
	note = {arXiv:2111.14819 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:/home/sspiegel/Zotero/storage/GWIRADGA/Yu et al. - 2022 - Point-BERT Pre-training 3D Point Cloud Transformers with Masked Point Modeling.pdf:application/pdf;Snapshot:/home/sspiegel/Zotero/storage/E7HXCLPK/2111.html:text/html},
}
